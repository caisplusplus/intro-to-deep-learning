{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a3647bee-6c37-43ce-b70e-73b0ac767a37"
    }
   },
   "source": [
    "# Intro to Deep Learning: Neural Networks for Digit Classification, Using Keras\n",
    "---\n",
    "Today, we'll be using Keras to **train a neural network to classify handwritten digits**. To do this, we'll be taking advantage of the pre-labeled [**MNIST dataset**](https://en.wikipedia.org/wiki/MNIST_database), which contains 70,000 grayscale images of handwritten digits, along with their corresponding digit labels.\n",
    "\n",
    "Since we want our model to be able to perform well in the real world on **previously unseen data**, we'll only train it on 60,000 images, and then evaluate our model on the remaining 10,000.\n",
    "\n",
    "### Introductory Guides:\n",
    "* Workshop presentation slides: [here](https://docs.google.com/presentation/d/1RHnHtlQt9nWoIsmiDPNZ4aoWPIzRt1X3OFm304OfmbQ/edit?usp=sharing)\n",
    "* Getting started with Keras: https://keras.io/\n",
    "* Keras Sequential model introductory guide: https://keras.io/getting-started/sequential-model-guide/\n",
    "* Keras Sequential model documentation: https://keras.io/models/sequential/\n",
    "* Keras core layers: https://keras.io/layers/core/\n",
    "\n",
    "### Topic-Specific Resources/Documentation:\n",
    "* Numpy arrays: https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.ndarray.html\n",
    "* Activation Functions - ML Cheatsheet: http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n",
    "* Activation Functions - Keras Documentation: https://keras.io/activations/\n",
    "* Cost/Loss Functions - ML Cheatsheet: http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "* Cost/Loss Functions - Keras Documentation: https://keras.io/losses\n",
    "* Keras Optimizers (e.g. Gradient Descent): https://keras.io/optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "aa21eec0-d9e9-4575-8cec-408d8f044b20"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST dataset from Keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b04508f5-7038-4d57-a3ad-a8e1dd3e172e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape is (60000, 28, 28)\n",
      "Input type is <type 'numpy.ndarray'>\n",
      "Labels:\n",
      "[5 0 4 ..., 5 6 8]\n",
      "Labels shape is (60000,)\n",
      "Labels type is <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "\n",
    "print(\"Inputs shape is \" + str(train_x.shape)) # 60,000 samples, each image: 28 x 28 pixels\n",
    "print(\"Input type is \" + str(type(train_x)))\n",
    "print(\"Labels:\")\n",
    "print(train_y)\n",
    "print(\"Labels shape is \" + str(train_y.shape))\n",
    "print(\"Labels type is \" + str(type(train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4f9fadce-17a5-45d3-abeb-4c09ff57cfa7"
    }
   },
   "outputs": [],
   "source": [
    "# Matplotlib: Data visualization library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "572a96a1-19cb-45e3-9763-acfc3d87dc7f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b3c1cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the input samples\n",
    "\n",
    "sample_num = 0 # change this number and re-run the cell to see different image samples!\n",
    "\n",
    "plt.imshow(train_x[sample_num], cmap=plt.get_cmap('gray'))\n",
    "print(train_y[sample_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data: \n",
    "* **Flatten** the 28 x 28 2D images into 784-dimensional column vectors. Each pixel will then correspond to one neuron in the 784-dimensional input layer of our neural network.\n",
    "* **Normalize** the pixel values from 0-255 to 0-1. We can do this by simply dividing each of the 0-255 greyscale values by 255. Neural networks typically like to work with smaller values, so this normalization is a pretty common first step in most deep learning tasks.\n",
    "* **Categorize** the outputs into 10-dimensional \"one-hot\" vectors. The MNIST dataset originally contains actual numerical labels for each image (e.g. 1, 2, ...), but remember that our neural network outputs 10 distinct values (one for each digit) -- not just the digit number itself. We want our training labels to match up with our neural network output. These categorized vectors contain all 0's, except a 1 in the location indicating which digit the image corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f8dd8606-82f9-4d0c-a62b-45beb371eddc"
    }
   },
   "outputs": [],
   "source": [
    "# Flatten 28*28 images to a 784 vector for each image\n",
    "\n",
    "num_pixels = train_x.shape[1] * train_x.shape[2] # 28 * 28 = 784\n",
    "train_x_flattened = train_x.reshape(train_x.shape[0], num_pixels).astype('float32') # new shape: 60,000 x 784\n",
    "test_x_flattened = test_x.reshape(test_x.shape[0], num_pixels).astype('float32') # new shape: 10,000 x 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "23cd5489-7e90-47a1-aba3-2a802d1075ac"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to between 0-1\n",
    "train_x_flattened = train_x_flattened / 255.\n",
    "test_x_flattened = test_x_flattened / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "e11182b1-7e19-4c34-be1e-b1c71179f014"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Use Keras to categorize the outputs (\"one-hot\" vectors)\n",
    "train_y_categorical = keras.utils.to_categorical(train_y, num_classes=10)\n",
    "test_y_categorical = keras.utils.to_categorical(test_y, num_classes=10)\n",
    "\n",
    "# let's see result of categorizing the outputs\n",
    "print(train_y_categorical[:5]) # print out first 5 training label vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Neural Network Model\n",
    "1. **Initialize** the network, add desired layers. The settings we decide to use, e.g. number of layers, number of neurons per layer, are called **hyperparameters**, and have to be tuned by hand, rather than learned via gradient descent.\n",
    "2. **Compile** the network to get ready for training. This tells the network what cost/loss function to use (\"cost\" and \"loss\" are used interchangeably), and what type of gradient descent to use.\n",
    "3. **Fit** the network to the training images. This actually feeds the training data into the network, and uses gradient descent and backpropagation to adjust the network's weights in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "81b9fc81-511c-4d2d-be23-883d0009bc74"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Initialize simple neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# NOTE: This model contains some tweaks from the original 'incomplete' notebook in order to improve performance.\n",
    "    # 1. Use 'relu' activation instead of 'sigmoid' for hidden layer\n",
    "    # 2. Use 'softmax' activation for final output layer\n",
    "    # 3. Use 'categorical crossentropy' loss function, instead of 'mean-squared-error'\n",
    "    # See the links provided below for more info\n",
    "\n",
    "# Hidden layer 1: 64 neurons, 'relu' activation \n",
    "    # (see: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "    # http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n",
    "model.add(Dense(units=64, input_dim=784, activation='relu'))\n",
    "\n",
    "# Hidden layer 2: 32 neurons, 'relu' activation\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# Output layer: 10 neurons (one for each class), 'softmax' activation\n",
    "    # This layer represents the scores that the network assigns to each possible digit, 1-10\n",
    "    # See: http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/\n",
    "model.add(Dense(units=10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bf842b6a-20db-42f4-9333-b1e30857e725"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model, get ready to train\n",
    "\n",
    "    # Loss: Categorical Crossentropy\n",
    "        # See: http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "    # Optimizer: stochastic gradient descent (SGD)\n",
    "    # Additional metrics: Accuracy\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 52,650\n",
      "Trainable params: 52,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "2fbdd2f4-e1eb-4bd5-aef4-5869948e4b89"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.4830 - acc: 0.8683    \n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2400 - acc: 0.9318    \n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s - loss: 0.1904 - acc: 0.9455     \n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.1583 - acc: 0.9541     \n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.1354 - acc: 0.9608     \n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 9s - loss: 0.1179 - acc: 0.9667     \n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.1049 - acc: 0.9698    \n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0943 - acc: 0.9726    \n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0852 - acc: 0.975 - 10s - loss: 0.0853 - acc: 0.9750    \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0778 - acc: 0.9777    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b446e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "\n",
    "# Train the model\n",
    "    # Number of epochs: 10 (i.e. how many times to loop over the training data)\n",
    "    # Batch size: 16 (how big our \"drunk walk\" samples should be)\n",
    "    # See: 'fit()' in https://keras.io/models/sequential/\n",
    "    \n",
    "model.fit(train_x_flattened, train_y_categorical, epochs=10, batch_size=16)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "0864543a-2392-4ae5-bbdd-375e934184f4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8064/10000 [=======================>......] - ETA: 0s()\n",
      "('Final test cost: ', 0.098051422105729577)\n",
      "('Final test accuracy: ', 0.97109999999999996)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate trained model on test data\n",
    "\n",
    "# Returns final test loss & test accuracy\n",
    "    # See: 'evaluate' in https://keras.io/models/sequential/\n",
    "loss_and_metrics = model.evaluate(test_x_flattened, test_y_categorical, batch_size=128)\n",
    "final_cost = loss_and_metrics[0]\n",
    "final_accuracy = loss_and_metrics[1]\n",
    "\n",
    "print()\n",
    "print(\"Final test cost: \", final_cost)\n",
    "print(\"Final test accuracy: \", final_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Sanity Check:** Make sure the our neural network's predictions match up with the actual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "3cdfe0ff-f08a-4ded-af33-c091211b22a7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Output vector: ', array([  9.99697328e-01,   5.96056964e-08,   2.26069285e-04,\n",
      "         1.68353509e-07,   1.35627394e-08,   2.60624370e-06,\n",
      "         6.07639631e-05,   1.78718125e-07,   1.36605721e-07,\n",
      "         1.26789255e-05], dtype=float32))\n",
      "('Predicted digit: ', 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8E9R8JmxpgouxFLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1054d7190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_num = 10 # which test sample to look at. TODO: Play around with this number to see how \\\n",
    "    # our neural network performs on different test images\n",
    "\n",
    "# Predicted class\n",
    "test_sample = np.expand_dims(test_x_flattened[sample_num], axis=0) # pick out a one-sample \"batch\" to feed into model\n",
    "predicted_scores = model.predict(test_sample) # outputted probabilities vector\n",
    "print(\"Output vector: \", predicted_scores[0]) # print predicted scores\n",
    "\n",
    "predicted_class = np.argmax(predicted_scores) # pick the class with highest probability --> final prediction\n",
    "print(\"Predicted digit: \", predicted_class) # print predicted classification\n",
    "\n",
    "# Show actual input image\n",
    "plt.imshow(test_x[sample_num], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Congrats! You just did deep learning!\n",
    "If you have extra time, feel free to play around with the hyperparameters (number of neurons per layer, number of epochs, batch size, etc.) to see if you can improve the network's final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
